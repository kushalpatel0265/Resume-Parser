{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "iVKXKWCAL-cA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 nltk\n",
        "!pip install llama-index-llms-groq llama-index\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8iBS-SADCKz",
        "outputId": "6f2e96a9-d41c-4a48-cbb8-74d06a7b0d88"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: llama-index-llms-groq in /usr/local/lib/python3.11/dist-packages (0.3.1)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (0.12.26)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-groq) (0.12.27)\n",
            "Requirement already satisfied: llama-index-llms-openai-like<0.4.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-groq) (0.3.4)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.6)\n",
            "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.1)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.6.9)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.29)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.7)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.68.2)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (2.0.39)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (3.11.14)\n",
            "Requirement already satisfied: banks<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (2.1.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.17)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-groq) (4.50.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.4.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.4.post1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (1.18.3)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (1.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (3.1.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (0.14.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.4 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.9)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (3.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-groq) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-groq) (0.29.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-groq) (24.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-groq) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like<0.4.0,>=0.3.1->llama-index-llms-groq) (0.5.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (3.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=4.3.7 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (4.3.7)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-groq) (3.0.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.50.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.29.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports, Setup, and Basic Variables"
      ],
      "metadata": {
        "id": "LZPlFxTuMG4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "4mO3njXYM4gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import re\n",
        "import nltk\n",
        "import PyPDF2\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "-odGJfQ1DFLO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK downloads"
      ],
      "metadata": {
        "id": "q14dy6u0M_fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"punkt_tab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyXDHQg4Mf2D",
        "outputId": "5ab09a00-4329-48b4-cef8-bc3f632d73a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt to import Groq LLM"
      ],
      "metadata": {
        "id": "Xe-lZUsnNE82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from llama_index.llms.groq import Groq\n",
        "    from llama_index.core.llms import ChatMessage\n",
        "    USE_GROQ = True\n",
        "except ImportError:\n",
        "    USE_GROQ = False"
      ],
      "metadata": {
        "id": "AA-Am-uFMid1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For partial code-based parse"
      ],
      "metadata": {
        "id": "mH7sPrBWNJPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Y0Vxd2TWMvCO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For semantic matching"
      ],
      "metadata": {
        "id": "08DBzmVYNNM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from sentence_transformers import SentenceTransformer, util\n",
        "    USE_SEMANTIC = True\n",
        "except ImportError:\n",
        "    USE_SEMANTIC = False"
      ],
      "metadata": {
        "id": "eMgnr-i7Mwql"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Groq API key"
      ],
      "metadata": {
        "id": "9wAg4YrLNQnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY = \"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\""
      ],
      "metadata": {
        "id": "VcJUg7EOMyeB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A small list of known skills for code-based detection"
      ],
      "metadata": {
        "id": "KIiVD4wxNZMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNOWN_SKILLS = [\"python\", \"java\", \"aws\", \"machine learning\", \"html\", \"css\", \"javascript\"]"
      ],
      "metadata": {
        "id": "5Gcl88EtM2A2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "Q70AOhbzMOVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1) Resume Parsing"
      ],
      "metadata": {
        "id": "Ic6nHPEYNrM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_file) -> str:\n",
        "    \"\"\"\n",
        "    Extract text from PDF using PyPDF2.\n",
        "    \"\"\"\n",
        "    reader = PyPDF2.PdfReader(pdf_file)\n",
        "    all_text = []\n",
        "    for page in reader.pages:\n",
        "        text = page.extract_text() or \"\"\n",
        "        all_text.append(text)\n",
        "    return \"\\n\".join(all_text)"
      ],
      "metadata": {
        "id": "-9h6HDerNqc5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def code_based_parse(resume_text: str, known_skills=None) -> dict:\n",
        "    \"\"\"\n",
        "    1) Regex for phone/email\n",
        "    2) Basic skill detection\n",
        "    3) Return partial parse dict\n",
        "    \"\"\"\n",
        "    phone_pattern = r'\\+?\\d[\\d\\s\\-.()]{7,}\\d'\n",
        "    phones = re.findall(phone_pattern, resume_text)\n",
        "\n",
        "    email_pattern = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
        "    emails = re.findall(email_pattern, resume_text)\n",
        "\n",
        "    detected_skills = []\n",
        "    if known_skills:\n",
        "        lower_txt = resume_text.lower()\n",
        "        for skill in known_skills:\n",
        "            if skill.lower() in lower_txt:\n",
        "                detected_skills.append(skill)\n",
        "\n",
        "    partial_data = {\n",
        "        \"phones\": list(set(phones)),\n",
        "        \"emails\": list(set(emails)),\n",
        "        \"skills\": list(set(detected_skills))\n",
        "    }\n",
        "    return partial_data"
      ],
      "metadata": {
        "id": "r9mT2uNfDG4Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finalize_summary_with_llm(resume_text: str, partial_data: dict) -> str:\n",
        "    \"\"\"\n",
        "    LLM verifies the partial parse, removing incorrect info,\n",
        "    finalizes the summary with subpoints like Education, Experience, etc.\n",
        "    If LLM not available or invalid key -> fallback message.\n",
        "    \"\"\"\n",
        "    if not USE_GROQ:\n",
        "        return \"Groq LLM not installed. Partial parse:\\n\" + str(partial_data)\n",
        "\n",
        "    system_msg = \"You finalize resume data, removing incorrect info, returning subpoints like Education, Experience, Skills, etc.\"\n",
        "    user_msg = f\"\"\"\n",
        "Here is a partial parse from code logic:\n",
        "Phones: {partial_data.get(\"phones\", [])}\n",
        "Emails: {partial_data.get(\"emails\", [])}\n",
        "Skills: {partial_data.get(\"skills\", [])}\n",
        "\n",
        "Resume text:\n",
        "{resume_text}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1) Verify these fields. If any are incorrect, remove or correct them.\n",
        "2) Summarize the entire resume in subpoints: Education, Experience, Projects, Skills, etc.\n",
        "3) Return the final summary in bullet points.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        llm = Groq(model=\"llama-3.1-8b-instant\", api_key=GROQ_API_KEY)\n",
        "        messages = [\n",
        "            ChatMessage(role=\"system\", content=system_msg),\n",
        "            ChatMessage(role=\"user\", content=user_msg),\n",
        "        ]\n",
        "        response = llm.chat(messages)\n",
        "        return str(response)\n",
        "    except Exception as e:\n",
        "        return f\"LLM call failed: {e}\\nPartial parse data:\\n{partial_data}\""
      ],
      "metadata": {
        "id": "QOCpjQo7N3WL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Matching Approaches"
      ],
      "metadata": {
        "id": "Zm24McxmOGga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemma_tokenize(text: str) -> set:\n",
        "    \"\"\"\n",
        "    Tokenize -> remove stopwords -> lemmatize -> set.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    sents = nltk.sent_tokenize(text)\n",
        "    lemmas = set()\n",
        "    for s in sents:\n",
        "        tokens = nltk.word_tokenize(s)\n",
        "        for tok in tokens:\n",
        "            tok_lower = tok.lower()\n",
        "            if tok_lower.isalnum() and tok_lower not in stop_words:\n",
        "                lemmas.add(lemmatizer.lemmatize(tok_lower))\n",
        "    return lemmas"
      ],
      "metadata": {
        "id": "V1Yf8ZReNjmO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemma_based_match(jd_text: str, resume_text: str) -> dict:\n",
        "    \"\"\"\n",
        "    1) Compute Jaccard similarity in [0..100]\n",
        "    2) Return matched/unmatched tokens\n",
        "    \"\"\"\n",
        "    jd_lemmas = lemma_tokenize(jd_text)\n",
        "    resume_lemmas = lemma_tokenize(resume_text)\n",
        "\n",
        "    if not jd_lemmas or not resume_lemmas:\n",
        "        return {\"score\": 0.0, \"matched\": [], \"jd_only\": [], \"resume_only\": []}\n",
        "\n",
        "    intersection = jd_lemmas & resume_lemmas\n",
        "    union = jd_lemmas | resume_lemmas\n",
        "    jaccard = (len(intersection) / len(union)) * 100\n",
        "    return {\n",
        "        \"score\": round(jaccard, 2),\n",
        "        \"matched\": sorted(list(intersection)),\n",
        "        \"jd_only\": sorted(list(jd_lemmas - intersection)),\n",
        "        \"resume_only\": sorted(list(resume_lemmas - intersection))\n",
        "    }"
      ],
      "metadata": {
        "id": "5HbtOkCiOLVB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_match(jd_text: str, resume_text: str, threshold=0.4) -> dict:\n",
        "    \"\"\"\n",
        "    1) Overall text similarity\n",
        "    2) Sentence-level from JD to Resume\n",
        "    \"\"\"\n",
        "    if not USE_SEMANTIC:\n",
        "        return {\n",
        "            \"overall_score\": 0.0,\n",
        "            \"matched_sents\": [],\n",
        "            \"unmatched_sents\": [],\n",
        "            \"note\": \"No sentence-transformers installed\"\n",
        "        }\n",
        "\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    # Overall\n",
        "    jd_emb = model.encode(jd_text, convert_to_tensor=True)\n",
        "    res_emb = model.encode(resume_text, convert_to_tensor=True)\n",
        "    sim_val = float(util.cos_sim(jd_emb, res_emb)[0][0])\n",
        "    overall_score = round(sim_val * 100, 2)\n",
        "\n",
        "    # Sentence-level\n",
        "    matched_sents = []\n",
        "    unmatched_sents = []\n",
        "    lines = nltk.sent_tokenize(jd_text)\n",
        "    for line in lines:\n",
        "        line_emb = model.encode(line, convert_to_tensor=True)\n",
        "        score_f = float(util.cos_sim(line_emb, res_emb)[0][0])\n",
        "        score_pct = round(score_f * 100, 2)\n",
        "        if score_f >= threshold:\n",
        "            matched_sents.append((line, score_pct))\n",
        "        else:\n",
        "            unmatched_sents.append((line, score_pct))\n",
        "\n",
        "    return {\n",
        "        \"overall_score\": overall_score,\n",
        "        \"matched_sents\": matched_sents,\n",
        "        \"unmatched_sents\": unmatched_sents\n",
        "    }"
      ],
      "metadata": {
        "id": "wg_cKxkEORx0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_score(lemma_val: float, sem_val: float, lemma_weight=0.5, sem_weight=0.5) -> float:\n",
        "    \"\"\"\n",
        "    Weighted average of lemma-based + semantic approach\n",
        "    \"\"\"\n",
        "    return round((lemma_weight * lemma_val) + (sem_weight * sem_val), 2)"
      ],
      "metadata": {
        "id": "c8J6kqEvOWhT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Code Execution"
      ],
      "metadata": {
        "id": "ig7Ol1abMV4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # 1) Prompt user for JD\n",
        "    print(\"Paste your job description below, then press Enter:\")\n",
        "    jd_text = input().strip()\n",
        "    if not jd_text:\n",
        "        print(\"No job description provided. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # 2) Ask user to upload PDF resume\n",
        "    print(\"Please upload your PDF resume now in the Colab file upload widget.\")\n",
        "    uploaded_files = files.upload()\n",
        "    if not uploaded_files:\n",
        "        print(\"No file uploaded. Exiting.\")\n",
        "        return\n",
        "\n",
        "    for filename in uploaded_files.keys():\n",
        "        if not filename.lower().endswith(\".pdf\"):\n",
        "            print(f\"Skipping file '{filename}' since it's not a PDF.\")\n",
        "            continue\n",
        "\n",
        "        pdf_stream = io.BytesIO(uploaded_files[filename])\n",
        "        resume_raw = extract_text_from_pdf(pdf_stream)\n",
        "\n",
        "        print(\"\\n--- RAW Resume Text ---\\n\")\n",
        "        print(resume_raw)\n",
        "\n",
        "        # 3) Code-based partial parse\n",
        "        partial_data = code_based_parse(resume_raw, known_skills=KNOWN_SKILLS)\n",
        "        print(\"\\n--- Partial Parse (Code-based) ---\\n\", partial_data)\n",
        "\n",
        "        # 4) LLM finalization\n",
        "        final_summary = finalize_summary_with_llm(resume_raw, partial_data)\n",
        "        print(\"\\n--- Finalized Summary (LLM) ---\\n\")\n",
        "        print(final_summary)\n",
        "\n",
        "        # 5) Lemma-based match\n",
        "        lemma_result = lemma_based_match(jd_text, resume_raw)\n",
        "        lemma_score = lemma_result[\"score\"]\n",
        "        matched = lemma_result[\"matched\"]\n",
        "        jd_only = lemma_result[\"jd_only\"]\n",
        "        res_only = lemma_result[\"resume_only\"]\n",
        "\n",
        "        print(\"\\n--- Lemma-Based Matching ---\\n\")\n",
        "        print(f\"Score: {lemma_score}%\")\n",
        "        print(\"Matched tokens:\", matched)\n",
        "        print(\"JD-only tokens:\", jd_only)\n",
        "        print(\"Resume-only tokens:\", res_only)\n",
        "\n",
        "        # 6) Semantic match\n",
        "        sem_result = semantic_match(jd_text, resume_raw, threshold=0.4)\n",
        "        sem_score = sem_result[\"overall_score\"]\n",
        "        matched_sents = sem_result[\"matched_sents\"]\n",
        "        unmatched_sents = sem_result[\"unmatched_sents\"]\n",
        "\n",
        "        print(\"\\n--- Semantic Matching ---\\n\")\n",
        "        print(f\"Overall score: {sem_score}%\")\n",
        "        print(\"\\nMatched JD Sentences:\")\n",
        "        for line, sc in matched_sents:\n",
        "            print(f\"- {line} ({sc}%)\")\n",
        "        print(\"\\nUnmatched JD Sentences:\")\n",
        "        for line, sc in unmatched_sents:\n",
        "            print(f\"- {line} ({sc}%)\")\n",
        "\n",
        "        # 7) Combined Score\n",
        "        combo = combined_score(lemma_score, sem_score, lemma_weight=0.5, sem_weight=0.5)\n",
        "        print(f\"\\n--- Combined Matching Score: {combo}% ---\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V-wfSv2IDN61",
        "outputId": "bfaa73dd-37ae-4702-e15f-3214e4c59923"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your job description below, then press Enter:\n",
            "Job Title: Full Stack Software Engineer  Overview: We are seeking a Full Stack Software Engineer to join our dynamic team. In this role, you will design, develop, and maintain modern web applications while collaborating closely with cross-functional teams. You will work on both front-end and back-end components and, when needed, integrate data analytics or machine learning elements to enhance our products.  Key Responsibilities:  Web Application Development: Develop, test, and deploy responsive web applications using modern frameworks (e.g., React, Next.js, Node.js, Django).  Back-End & Database Management: Design and maintain scalable back-end systems and databases (SQL and NoSQL) for efficient data storage and retrieval.  Cloud & DevOps: Leverage cloud platforms (AWS, Azure) for application deployment and scalability, and work with CI/CD pipelines to ensure robust and continuous delivery.  Data & Machine Learning Integration: When required, integrate data analytics and machine learning components to support intelligent features in our applications.  Collaboration & Communication: Work closely with design, product, and data teams to gather requirements, refine specifications, and deliver high-quality software solutions.  Required Qualifications:  Bachelor’s degree in Computer Science, Engineering, or a related field.  Proven experience in full-stack web development with strong proficiency in languages such as Python, JavaScript, or Java.  Solid understanding of both front-end (HTML, CSS, JavaScript frameworks) and back-end technologies (Node.js, Django, etc.).  Experience designing RESTful APIs and working with modern database systems.  Excellent problem-solving skills and ability to work collaboratively in an agile environment.  Preferred Qualifications:  Experience with machine learning or data analytics projects.  Familiarity with content management systems (e.g., AEM, Drupal, WordPress).  Understanding of version control systems (e.g., Git) and agile development practices.\n",
            "Please upload your PDF resume now in the Colab file upload widget.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0c0ea3d1-ced1-48ed-9e25-a6f9331c84c7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0c0ea3d1-ced1-48ed-9e25-a6f9331c84c7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Kushal Patel.pdf to Kushal Patel (3).pdf\n",
            "\n",
            "--- RAW Resume Text ---\n",
            "\n",
            "Kushal Patel\n",
            "Vadodara, India\n",
            "/ne+91-9879580177 kushalpatel0265@gmail.com Linked In /gtbGitHub /cdeCodeforces /cdeLeetcode\n",
            "EDUCATION\n",
            "Nirma University 10/2022 – Present\n",
            "B.Tech Computer Science and Engineering - Percentage -8.23 Ahmedabad, India\n",
            "Parth School of Science and Competition 06/2020 – 06/2022\n",
            "Higher Secondary - Science - Percentage -90% Vadodara, India\n",
            "EXPERIENCE\n",
            "Dhyey Consultancy 05/2024 – 07/2024\n",
            "Machine Learning Intern Vadodara, India\n",
            "•Executed SQL queries on a music database, analyzing 10,000+ records to identify sales trends and track\n",
            "popularity across genres.\n",
            "•Built detailed reports correlating track plays with sales, influencing promotional strategies.\n",
            "•Improved marketing by leveraging analysis results to refine promotional tactics.\n",
            "PROJECTS\n",
            "Code Craft |Next.js, Convex, Clerk, TypeScript 01/2024\n",
            "•Implemented a SaaS code editor with Next.js, Convex, Clerk, and TypeScript, supporting 10 programming\n",
            "languages, 5 VSCode themes, advanced filtering, personal profiles (execution history), a statistics dashboard,\n",
            "and customizable font sizes.\n",
            "•Incorporated smart output handling, community-driven code sharing, and flexible pricing (Free & Pro) with\n",
            "Lemon Squeezy for payment processing.\n",
            "Advanced Authentication |MERN & Next.js 12/2024\n",
            "•Built with MERN & Next.js for robust authentication, server-side rendering, and improved performance.\n",
            "•Features secure user registration, JWT-based auth, OAuth (Google & GitHub), email verification, password\n",
            "resets, and protected routes.\n",
            "Pneumonia Detection Web Application |Python, TensorFlow, Streamlit, PIL 07/2024\n",
            "•Developed a web-based pneumonia detection application using TensorFlow and Streamlit, enabling instant\n",
            "analysis and diagnosis from X-ray images.\n",
            "•Trained on 624 X-rays (234 normal and 390 pneumonia) and tested on 4,816 X-rays (1,341 normal and 3,475\n",
            "pneumonia), ensuring high accuracy and reliability.\n",
            "•Created a user-friendly interface with robust file management and error handling; the model achieves around\n",
            "85% accuracy in predictions.\n",
            "Airport Management System |SQL 04/2024\n",
            "•Designed and normalized an ER diagram for an Airport Management System, transforming it into a relational\n",
            "model with 10 SQL tables and 100 records each to reflect real-world operations.\n",
            "•Managed SQL connections and executed 30 queries for data manipulation, updates, and reporting, showcasing\n",
            "database management proficiency.\n",
            "TECHNICAL SKILLS\n",
            "Skills : Data Structures and Algorithms, Object Oriented Programming (OOP), Computer Networks, Operating\n",
            "System, Database Management System, Machine Learning\n",
            "Languages : C, C++, Java, Python, SQL, HTML, CSS, JavaScript, TypeScript\n",
            "Framework/Libraries : Numpy, Pandas, Matplotlib, Node.js, Next.js, ExpressJs, Tableau, Clerk\n",
            "Operating System : Windows, Linux, Mac\n",
            "Database : MySQL, MongoDB, Convex\n",
            "Soft skills : Teamwork, Communication, Leadership\n",
            "CERTIFICATIONS\n",
            "•Machine Learning with Python\n",
            "•Problem Solving\n",
            "•SQL\n",
            "\n",
            "--- Partial Parse (Code-based) ---\n",
            " {'phones': ['+91-9879580177'], 'emails': ['kushalpatel0265@gmail.com'], 'skills': ['javascript', 'machine learning', 'python', 'html', 'java', 'css']}\n",
            "\n",
            "--- Finalized Summary (LLM) ---\n",
            "\n",
            "assistant: **Verified Resume Data:**\n",
            "\n",
            "1. **Phones:** ['+91-9879580177']\n",
            "   - Verified: Correct\n",
            "\n",
            "2. **Emails:** ['kushalpatel0265@gmail.com']\n",
            "   - Verified: Correct\n",
            "\n",
            "3. **Skills:** ['javascript', 'machine learning', 'python', 'html', 'java', 'css']\n",
            "   - Verified: Correct\n",
            "\n",
            "**Resume Summary:**\n",
            "\n",
            "* **Education:**\n",
            "  • Nirma University (10/2022 – Present)\n",
            "    - B.Tech Computer Science and Engineering\n",
            "    - Percentage: 8.23\n",
            "    - Location: Ahmedabad, India\n",
            "  • Parth School of Science and Competition (06/2020 – 06/2022)\n",
            "    - Higher Secondary - Science\n",
            "    - Percentage: 90%\n",
            "    - Location: Vadodara, India\n",
            "\n",
            "* **Experience:**\n",
            "  • Dhyey Consultancy (05/2024 – 07/2024)\n",
            "    - Machine Learning Intern\n",
            "    - Location: Vadodara, India\n",
            "    - Key Responsibilities:\n",
            "      - Executed SQL queries on a music database\n",
            "      - Built detailed reports correlating track plays with sales\n",
            "      - Improved marketing by leveraging analysis results\n",
            "\n",
            "* **Projects:**\n",
            "  • Code Craft (01/2024)\n",
            "    - Implemented a SaaS code editor with Next.js, Convex, Clerk, and TypeScript\n",
            "    - Features: 10 programming languages, 5 VSCode themes, advanced filtering, personal profiles, and customizable font sizes\n",
            "  • Advanced Authentication (12/2024)\n",
            "    - Built with MERN & Next.js for robust authentication, server-side rendering, and improved performance\n",
            "    - Features: secure user registration, JWT-based auth, OAuth, email verification, password resets, and protected routes\n",
            "  • Pneumonia Detection Web Application (07/2024)\n",
            "    - Developed a web-based pneumonia detection application using TensorFlow and Streamlit\n",
            "    - Features: instant analysis and diagnosis from X-ray images, user-friendly interface, robust file management, and error handling\n",
            "  • Airport Management System (04/2024)\n",
            "    - Designed and normalized an ER diagram for an Airport Management System\n",
            "    - Features: relational model with 10 SQL tables and 100 records each, SQL connections, and data manipulation, updates, and reporting\n",
            "\n",
            "* **Skills:**\n",
            "  • Technical Skills: Data Structures and Algorithms, Object Oriented Programming (OOP), Computer Networks, Operating System, Database Management System, Machine Learning\n",
            "  • Programming Languages: C, C++, Java, Python, SQL, HTML, CSS, JavaScript, TypeScript\n",
            "  • Framework/Libraries: Numpy, Pandas, Matplotlib, Node.js, Next.js, ExpressJs, Tableau, Clerk\n",
            "  • Operating System: Windows, Linux, Mac\n",
            "  • Database: MySQL, MongoDB, Convex\n",
            "  • Soft Skills: Teamwork, Communication, Leadership\n",
            "\n",
            "* **Certifications:**\n",
            "  • Machine Learning with Python\n",
            "  • Problem Solving\n",
            "  • SQL\n",
            "\n",
            "--- Lemma-Based Matching ---\n",
            "\n",
            "Score: 9.19%\n",
            "Matched tokens: ['application', 'communication', 'computer', 'cs', 'data', 'database', 'engineering', 'experience', 'html', 'java', 'javascript', 'language', 'learning', 'machine', 'management', 'proficiency', 'project', 'python', 'refine', 'robust', 'science', 'skill', 'sql', 'system', 'using', 'web']\n",
            "JD-only tokens: ['ability', 'aem', 'agile', 'analytics', 'apis', 'aws', 'azure', 'bachelor', 'closely', 'cloud', 'collaborating', 'collaboration', 'collaboratively', 'component', 'content', 'continuous', 'control', 'degree', 'deliver', 'delivery', 'deploy', 'deployment', 'design', 'designing', 'develop', 'development', 'devops', 'django', 'drupal', 'dynamic', 'efficient', 'element', 'engineer', 'enhance', 'ensure', 'environment', 'etc', 'excellent', 'familiarity', 'feature', 'field', 'framework', 'full', 'gather', 'git', 'integrate', 'integration', 'intelligent', 'job', 'join', 'key', 'leverage', 'maintain', 'modern', 'needed', 'nosql', 'overview', 'pipeline', 'platform', 'practice', 'preferred', 'product', 'proven', 'qualification', 'react', 'related', 'required', 'requirement', 'responsibility', 'responsive', 'restful', 'retrieval', 'role', 'scalability', 'scalable', 'seeking', 'software', 'solid', 'solution', 'specification', 'stack', 'storage', 'strong', 'support', 'team', 'technology', 'test', 'title', 'understanding', 'version', 'wordpress', 'work', 'working']\n",
            "Resume-only tokens: ['10', '100', '234', '30', '390', '5', '624', '85', 'accuracy', 'achieves', 'across', 'advanced', 'ahmedabad', 'airport', 'algorithm', 'analysis', 'analyzing', 'around', 'auth', 'authentication', 'c', 'certification', 'clerk', 'code', 'competition', 'connection', 'consultancy', 'convex', 'correlating', 'craft', 'customizable', 'dashboard', 'detailed', 'detection', 'dhyey', 'diagnosis', 'diagram', 'editor', 'education', 'email', 'enabling', 'ensuring', 'er', 'error', 'executed', 'execution', 'expressjs', 'file', 'filtering', 'flexible', 'font', 'free', 'genre', 'github', 'google', 'handling', 'high', 'higher', 'history', 'identify', 'image', 'improved', 'india', 'influencing', 'instant', 'interface', 'intern', 'kushal', 'kushalpatel0265', 'leadership', 'lemon', 'leveraging', 'linked', 'linux', 'mac', 'manipulation', 'marketing', 'matplotlib', 'mern', 'model', 'mongodb', 'music', 'mysql', 'network', 'nirma', 'normal', 'normalized', 'numpy', 'oauth', 'object', 'oop', 'operating', 'operation', 'oriented', 'output', 'panda', 'parth', 'password', 'patel', 'payment', 'percentage', 'performance', 'personal', 'pil', 'play', 'pneumonia', 'popularity', 'prediction', 'present', 'pricing', 'pro', 'processing', 'profile', 'programming', 'promotional', 'protected', 'query', 'record', 'reflect', 'registration', 'relational', 'reliability', 'rendering', 'report', 'reporting', 'reset', 'result', 'route', 'saas', 'sale', 'school', 'secondary', 'secure', 'sharing', 'showcasing', 'size', 'smart', 'soft', 'solving', 'squeezy', 'statistic', 'strategy', 'streamlit', 'structure', 'supporting', 'table', 'tableau', 'tactic', 'teamwork', 'technical', 'tensorflow', 'tested', 'theme', 'track', 'transforming', 'trend', 'typescript', 'university', 'update', 'user', 'vadodara', 'verification', 'vscode', 'window']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Semantic Matching ---\n",
            "\n",
            "Overall score: 34.23%\n",
            "\n",
            "Matched JD Sentences:\n",
            "- Data & Machine Learning Integration: When required, integrate data analytics and machine learning components to support intelligent features in our applications. (43.58%)\n",
            "\n",
            "Unmatched JD Sentences:\n",
            "- Job Title: Full Stack Software Engineer  Overview: We are seeking a Full Stack Software Engineer to join our dynamic team. (27.65%)\n",
            "- In this role, you will design, develop, and maintain modern web applications while collaborating closely with cross-functional teams. (26.96%)\n",
            "- You will work on both front-end and back-end components and, when needed, integrate data analytics or machine learning elements to enhance our products. (39.14%)\n",
            "- Key Responsibilities:  Web Application Development: Develop, test, and deploy responsive web applications using modern frameworks (e.g., React, Next.js, Node.js, Django). (19.28%)\n",
            "- Back-End & Database Management: Design and maintain scalable back-end systems and databases (SQL and NoSQL) for efficient data storage and retrieval. (27.1%)\n",
            "- Cloud & DevOps: Leverage cloud platforms (AWS, Azure) for application deployment and scalability, and work with CI/CD pipelines to ensure robust and continuous delivery. (10.96%)\n",
            "- Collaboration & Communication: Work closely with design, product, and data teams to gather requirements, refine specifications, and deliver high-quality software solutions. (24.13%)\n",
            "- Required Qualifications:  Bachelor’s degree in Computer Science, Engineering, or a related field. (13.99%)\n",
            "- Proven experience in full-stack web development with strong proficiency in languages such as Python, JavaScript, or Java. (29.46%)\n",
            "- Solid understanding of both front-end (HTML, CSS, JavaScript frameworks) and back-end technologies (Node.js, Django, etc.). (25.3%)\n",
            "- Experience designing RESTful APIs and working with modern database systems. (29.24%)\n",
            "- Excellent problem-solving skills and ability to work collaboratively in an agile environment. (22.45%)\n",
            "- Preferred Qualifications:  Experience with machine learning or data analytics projects. (39.49%)\n",
            "- Familiarity with content management systems (e.g., AEM, Drupal, WordPress). (26.52%)\n",
            "- Understanding of version control systems (e.g., Git) and agile development practices. (13.86%)\n",
            "\n",
            "--- Combined Matching Score: 21.71% ---\n",
            "\n"
          ]
        }
      ]
    }
  ]
}